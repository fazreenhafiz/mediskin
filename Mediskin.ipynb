{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc59c43b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using ratios from Fold1 → train:0.839  val:0.144  test:0.017\n",
      "Normal split → train:341  val:59  test:7  (total 407)\n",
      "✅ Done. Final dataset at: D:\\psm2\\MediSkinDataset\n"
     ]
    }
   ],
   "source": [
    "import os, shutil, random\n",
    "from pathlib import Path\n",
    "\n",
    "# ==== EDIT THESE TO YOUR FOLDERS ====\n",
    "FOLD1_DIR = Path(r\"D:\\psm2\\Fold1\")     # has train/val/test for monkeypox & others\n",
    "NORMAL_DIR = Path(r\"D:\\psm2\\Normal\")   # a flat folder of Normal images (not split yet)\n",
    "OUTPUT_DIR = Path(r\"D:\\psm2\\MediSkinDataset\")  # final merged dataset\n",
    "CLASS_NAME = \"normal\"                  # folder name for the class\n",
    "\n",
    "# ---------- Helpers ----------\n",
    "IMG_EXT = {\".jpg\",\".jpeg\",\".png\",\".bmp\",\".tif\",\".tiff\",\".webp\"}\n",
    "\n",
    "def list_images(root: Path):\n",
    "    return [p for p in root.rglob(\"*\") if p.is_file() and p.suffix.lower() in IMG_EXT]\n",
    "\n",
    "def ensure_dir(p: Path):\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def copy_all(files, dst_dir: Path, prefix=\"normal\"):\n",
    "    ensure_dir(dst_dir)\n",
    "    for i, src in enumerate(files, 1):\n",
    "        dst = dst_dir / f\"{prefix}_{i:05d}{src.suffix.lower()}\"\n",
    "        shutil.copy2(src, dst)\n",
    "\n",
    "# ---------- 1) Read Fold1 ratios ----------\n",
    "# Count total images across train/val/test (use one existing class to infer ratios)\n",
    "def split_ratio_from_fold1():\n",
    "    # pick the first class we find under train to anchor ratios\n",
    "    train_classes = [d for d in (FOLD1_DIR/\"train\").iterdir() if d.is_dir()]\n",
    "    if not train_classes:\n",
    "        raise RuntimeError(\"No classes found under Fold1/train\")\n",
    "\n",
    "    anchor_class = train_classes[0].name\n",
    "    n_train = len(list_images(FOLD1_DIR/\"train\"/anchor_class))\n",
    "    n_val   = len(list_images(FOLD1_DIR/\"val\"/anchor_class)) if (FOLD1_DIR/\"val\").exists() else 0\n",
    "    n_test  = len(list_images(FOLD1_DIR/\"test\"/anchor_class))\n",
    "\n",
    "    total = n_train + n_val + n_test\n",
    "    if total == 0:\n",
    "        raise RuntimeError(\"Fold1 appears empty.\")\n",
    "\n",
    "    r_train = n_train / total\n",
    "    r_val   = n_val   / total\n",
    "    r_test  = n_test  / total\n",
    "    return r_train, r_val, r_test\n",
    "\n",
    "r_train, r_val, r_test = split_ratio_from_fold1()\n",
    "print(f\"Using ratios from Fold1 → train:{r_train:.3f}  val:{r_val:.3f}  test:{r_test:.3f}\")\n",
    "\n",
    "# ---------- 2) Collect Normal images and split ----------\n",
    "normal_imgs = [p for p in NORMAL_DIR.iterdir() if p.is_file() and p.suffix.lower() in IMG_EXT]\n",
    "if not normal_imgs:\n",
    "    raise RuntimeError(\"No images found in the Normal folder.\")\n",
    "\n",
    "random.seed(42)\n",
    "random.shuffle(normal_imgs)\n",
    "\n",
    "N = len(normal_imgs)\n",
    "n_train = int(round(r_train * N))\n",
    "n_val   = int(round(r_val   * N))\n",
    "# put the remainder in test to ensure totals add up\n",
    "n_test  = N - n_train - n_val\n",
    "\n",
    "train_files = normal_imgs[:n_train]\n",
    "val_files   = normal_imgs[n_train:n_train+n_val]\n",
    "test_files  = normal_imgs[n_train+n_val:]\n",
    "\n",
    "print(f\"Normal split → train:{len(train_files)}  val:{len(val_files)}  test:{len(test_files)}  (total {N})\")\n",
    "\n",
    "# ---------- 3) Build output structure ----------\n",
    "# First, mirror Fold1 into OUTPUT_DIR\n",
    "for split in [\"train\",\"val\",\"test\"]:\n",
    "    src = FOLD1_DIR/split\n",
    "    if not src.exists(): \n",
    "        continue\n",
    "    for cls_dir in [d for d in src.iterdir() if d.is_dir()]:\n",
    "        dst = OUTPUT_DIR/split/cls_dir.name\n",
    "        ensure_dir(dst)\n",
    "        # copy existing monkeypox/others (keep filenames)\n",
    "        for img in list_images(cls_dir):\n",
    "            shutil.copy2(img, dst/img.name)\n",
    "\n",
    "# Now add Normal into OUTPUT_DIR\n",
    "copy_all(train_files, OUTPUT_DIR/\"train\"/CLASS_NAME, prefix=\"normal\")\n",
    "if r_val > 0:\n",
    "    copy_all(val_files,   OUTPUT_DIR/\"val\"/CLASS_NAME,   prefix=\"normal\")\n",
    "copy_all(test_files,  OUTPUT_DIR/\"test\"/CLASS_NAME,  prefix=\"normal\")\n",
    "\n",
    "print(\"✅ Done. Final dataset at:\", OUTPUT_DIR)\n",
    "# You should now have:\n",
    "# MediSkinDataset/\n",
    "#   train/  monkeypox/ others/ normal/\n",
    "#   val/    monkeypox/ others/ normal/   (only if Fold1 had val)\n",
    "#   test/   monkeypox/ others/ normal/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89fbf334",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2483 images belonging to 3 classes.\n",
      "Found 479 images belonging to 3 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Fazreen\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.6068 - loss: 1.0126"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Fazreen\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m212s\u001b[0m 3s/step - accuracy: 0.6079 - loss: 1.0093 - val_accuracy: 0.7578 - val_loss: 0.4879 - learning_rate: 0.0010\n",
      "Epoch 2/25\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.8158 - loss: 0.4343"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m162s\u001b[0m 2s/step - accuracy: 0.8158 - loss: 0.4341 - val_accuracy: 0.7787 - val_loss: 0.4614 - learning_rate: 0.0010\n",
      "Epoch 3/25\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m149s\u001b[0m 2s/step - accuracy: 0.8416 - loss: 0.3812 - val_accuracy: 0.7557 - val_loss: 0.4654 - learning_rate: 0.0010\n",
      "Epoch 4/25\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m152s\u001b[0m 2s/step - accuracy: 0.8605 - loss: 0.3396 - val_accuracy: 0.7474 - val_loss: 0.4920 - learning_rate: 0.0010\n",
      "Epoch 5/25\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m122s\u001b[0m 2s/step - accuracy: 0.8739 - loss: 0.2988 - val_accuracy: 0.7599 - val_loss: 0.4951 - learning_rate: 0.0010\n",
      "Epoch 6/25\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.8821 - loss: 0.2635"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 2s/step - accuracy: 0.8821 - loss: 0.2633 - val_accuracy: 0.7954 - val_loss: 0.4230 - learning_rate: 3.0000e-04\n",
      "Epoch 7/25\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.8782 - loss: 0.2562"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 2s/step - accuracy: 0.8784 - loss: 0.2560 - val_accuracy: 0.7975 - val_loss: 0.4365 - learning_rate: 3.0000e-04\n",
      "Epoch 8/25\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.9067 - loss: 0.2235"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 2s/step - accuracy: 0.9067 - loss: 0.2235 - val_accuracy: 0.8079 - val_loss: 0.4292 - learning_rate: 3.0000e-04\n",
      "Epoch 9/25\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m128s\u001b[0m 2s/step - accuracy: 0.9114 - loss: 0.2097 - val_accuracy: 0.7704 - val_loss: 0.4695 - learning_rate: 3.0000e-04\n",
      "Epoch 10/25\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m137s\u001b[0m 2s/step - accuracy: 0.9010 - loss: 0.2028 - val_accuracy: 0.7808 - val_loss: 0.4528 - learning_rate: 9.0000e-05\n",
      "Epoch 11/25\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m125s\u001b[0m 2s/step - accuracy: 0.9284 - loss: 0.1755 - val_accuracy: 0.7808 - val_loss: 0.4547 - learning_rate: 9.0000e-05\n",
      "Epoch 12/25\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m134s\u001b[0m 2s/step - accuracy: 0.9171 - loss: 0.1982 - val_accuracy: 0.7641 - val_loss: 0.4784 - learning_rate: 9.0000e-05\n",
      "Epoch 1/10\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m171s\u001b[0m 2s/step - accuracy: 0.7374 - loss: 0.6494 - val_accuracy: 0.6639 - val_loss: 1.5658 - learning_rate: 1.0000e-04\n",
      "Epoch 2/10\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m150s\u001b[0m 2s/step - accuracy: 0.8785 - loss: 0.2611 - val_accuracy: 0.6973 - val_loss: 1.8569 - learning_rate: 1.0000e-04\n",
      "Epoch 3/10\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 2s/step - accuracy: 0.9187 - loss: 0.1980 - val_accuracy: 0.6889 - val_loss: 1.9803 - learning_rate: 1.0000e-04\n",
      "Epoch 4/10\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 2s/step - accuracy: 0.9410 - loss: 0.1542 - val_accuracy: 0.6743 - val_loss: 1.3893 - learning_rate: 1.0000e-04\n",
      "Epoch 5/10\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m131s\u001b[0m 2s/step - accuracy: 0.9473 - loss: 0.1313 - val_accuracy: 0.6889 - val_loss: 1.9195 - learning_rate: 1.0000e-04\n",
      "Epoch 6/10\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 1s/step - accuracy: 0.9566 - loss: 0.1036 - val_accuracy: 0.7161 - val_loss: 1.9039 - learning_rate: 1.0000e-04\n",
      "Epoch 7/10\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 843ms/step - accuracy: 0.9568 - loss: 0.1059 - val_accuracy: 0.6388 - val_loss: 3.9587 - learning_rate: 1.0000e-04\n",
      "Epoch 8/10\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 824ms/step - accuracy: 0.9622 - loss: 0.0941 - val_accuracy: 0.6430 - val_loss: 3.7581 - learning_rate: 3.0000e-05\n",
      "Epoch 9/10\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 841ms/step - accuracy: 0.9702 - loss: 0.0736 - val_accuracy: 0.6534 - val_loss: 3.8331 - learning_rate: 3.0000e-05\n",
      "Epoch 10/10\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 829ms/step - accuracy: 0.9672 - loss: 0.0816 - val_accuracy: 0.6555 - val_loss: 3.8832 - learning_rate: 3.0000e-05\n",
      "saved model to D:\\psm2\\models\\disease_mnv2.h5\n"
     ]
    }
   ],
   "source": [
    "# train_disease_mnv2.py\n",
    "import os, json, tensorflow as tf\n",
    "from tensorflow.keras.applications import mobilenet_v2\n",
    "from tensorflow.keras import layers, models, callbacks, optimizers\n",
    "\n",
    "DATA = r\"D:\\psm2\\MediSkinDataset\"\n",
    "OUT  = r\"D:\\psm2\\models\"; os.makedirs(OUT, exist_ok=True)\n",
    "IMG=(224,224); BATCH=32\n",
    "\n",
    "train_gen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    rescale=1./255, rotation_range=12, width_shift_range=0.05, height_shift_range=0.05,\n",
    "    zoom_range=0.15, horizontal_flip=True)\n",
    "val_gen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_ds = train_gen.flow_from_directory(f\"{DATA}\\\\train\", target_size=IMG, batch_size=BATCH, class_mode=\"categorical\")\n",
    "val_ds   = val_gen.flow_from_directory(f\"{DATA}\\\\val\",   target_size=IMG, batch_size=BATCH, class_mode=\"categorical\")\n",
    "\n",
    "with open(os.path.join(OUT,\"disease_class_indices.json\"),\"w\") as f:\n",
    "    json.dump(train_ds.class_indices, f, indent=2)\n",
    "\n",
    "base = mobilenet_v2.MobileNetV2(weights=\"imagenet\", include_top=False, input_shape=IMG+(3,))\n",
    "base.trainable=False\n",
    "model = models.Sequential([ base,\n",
    "    layers.GlobalAveragePooling2D(),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(256, activation=\"relu\"),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(train_ds.num_classes, activation=\"softmax\") ])\n",
    "\n",
    "model.compile(optimizer=optimizers.Adam(1e-3), loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "ckp = callbacks.ModelCheckpoint(r\"D:\\psm2\\models\\disease_mnv2.h5\", save_best_only=True, monitor=\"val_accuracy\")\n",
    "es  = callbacks.EarlyStopping(patience=6, restore_best_weights=True)\n",
    "rlr = callbacks.ReduceLROnPlateau(patience=3, factor=0.3)\n",
    "\n",
    "model.fit(train_ds, epochs=25, validation_data=val_ds, callbacks=[ckp, es, rlr])\n",
    "\n",
    "# optional fine-tune tail\n",
    "base.trainable=True\n",
    "for layer in base.layers[:-20]: layer.trainable=False\n",
    "model.compile(optimizer=optimizers.Adam(1e-4), loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "model.fit(train_ds, epochs=10, validation_data=val_ds, callbacks=[ckp, es, rlr])\n",
    "print(\"saved model to D:\\\\psm2\\\\models\\\\disease_mnv2.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16ace50e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2483 images belonging to 3 classes.\n",
      "Found 479 images belonging to 3 classes.\n",
      "Epoch 1/25\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.4129 - loss: 1.1872"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 2s/step - accuracy: 0.4132 - loss: 1.1858 - val_accuracy: 0.5511 - val_loss: 0.8617 - learning_rate: 0.0010\n",
      "Epoch 2/25\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.4667 - loss: 0.9639"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 2s/step - accuracy: 0.4669 - loss: 0.9637 - val_accuracy: 0.6973 - val_loss: 0.8142 - learning_rate: 0.0010\n",
      "Epoch 3/25\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 2s/step - accuracy: 0.4945 - loss: 0.9122 - val_accuracy: 0.6868 - val_loss: 0.7857 - learning_rate: 0.0010\n",
      "Epoch 4/25\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m131s\u001b[0m 2s/step - accuracy: 0.5205 - loss: 0.8991 - val_accuracy: 0.6430 - val_loss: 0.7565 - learning_rate: 0.0010\n",
      "Epoch 5/25\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m133s\u001b[0m 2s/step - accuracy: 0.5202 - loss: 0.8777 - val_accuracy: 0.6451 - val_loss: 0.7748 - learning_rate: 0.0010\n",
      "Epoch 6/25\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m133s\u001b[0m 2s/step - accuracy: 0.5374 - loss: 0.8755 - val_accuracy: 0.6618 - val_loss: 0.7657 - learning_rate: 0.0010\n",
      "Epoch 7/25\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 2s/step - accuracy: 0.5416 - loss: 0.8697 - val_accuracy: 0.6555 - val_loss: 0.7415 - learning_rate: 0.0010\n",
      "Epoch 8/25\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m132s\u001b[0m 2s/step - accuracy: 0.5542 - loss: 0.8777 - val_accuracy: 0.6618 - val_loss: 0.7671 - learning_rate: 0.0010\n",
      "Epoch 9/25\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m132s\u001b[0m 2s/step - accuracy: 0.5555 - loss: 0.8583 - val_accuracy: 0.6868 - val_loss: 0.7351 - learning_rate: 0.0010\n",
      "Epoch 10/25\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m131s\u001b[0m 2s/step - accuracy: 0.5457 - loss: 0.8524 - val_accuracy: 0.6618 - val_loss: 0.7519 - learning_rate: 0.0010\n",
      "Epoch 11/25\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m132s\u001b[0m 2s/step - accuracy: 0.5480 - loss: 0.8485 - val_accuracy: 0.6639 - val_loss: 0.7299 - learning_rate: 0.0010\n",
      "Epoch 12/25\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m129s\u001b[0m 2s/step - accuracy: 0.5477 - loss: 0.8522 - val_accuracy: 0.6200 - val_loss: 0.7931 - learning_rate: 0.0010\n",
      "Epoch 13/25\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m129s\u001b[0m 2s/step - accuracy: 0.5562 - loss: 0.8507 - val_accuracy: 0.6743 - val_loss: 0.7451 - learning_rate: 0.0010\n",
      "Epoch 14/25\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m129s\u001b[0m 2s/step - accuracy: 0.5376 - loss: 0.8623 - val_accuracy: 0.6367 - val_loss: 0.7743 - learning_rate: 0.0010\n",
      "Epoch 15/25\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m128s\u001b[0m 2s/step - accuracy: 0.5709 - loss: 0.8541 - val_accuracy: 0.6493 - val_loss: 0.7565 - learning_rate: 3.0000e-04\n",
      "Epoch 16/25\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m128s\u001b[0m 2s/step - accuracy: 0.5769 - loss: 0.8172 - val_accuracy: 0.6451 - val_loss: 0.7557 - learning_rate: 3.0000e-04\n",
      "Epoch 17/25\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m128s\u001b[0m 2s/step - accuracy: 0.5684 - loss: 0.8262 - val_accuracy: 0.6639 - val_loss: 0.7275 - learning_rate: 3.0000e-04\n",
      "Epoch 18/25\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1316s\u001b[0m 17s/step - accuracy: 0.5628 - loss: 0.8293 - val_accuracy: 0.6639 - val_loss: 0.7286 - learning_rate: 3.0000e-04\n",
      "Epoch 19/25\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m139s\u001b[0m 2s/step - accuracy: 0.5552 - loss: 0.8319 - val_accuracy: 0.6597 - val_loss: 0.7282 - learning_rate: 3.0000e-04\n",
      "Epoch 20/25\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 2s/step - accuracy: 0.5703 - loss: 0.8268 - val_accuracy: 0.6347 - val_loss: 0.7616 - learning_rate: 3.0000e-04\n",
      "Epoch 21/25\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 2s/step - accuracy: 0.5732 - loss: 0.8204 - val_accuracy: 0.6576 - val_loss: 0.7330 - learning_rate: 9.0000e-05\n",
      "Epoch 22/25\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 2s/step - accuracy: 0.5710 - loss: 0.8279 - val_accuracy: 0.6618 - val_loss: 0.7283 - learning_rate: 9.0000e-05\n",
      "Epoch 23/25\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 2s/step - accuracy: 0.5640 - loss: 0.8283 - val_accuracy: 0.6576 - val_loss: 0.7358 - learning_rate: 9.0000e-05\n",
      "Epoch 1/10\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m215s\u001b[0m 3s/step - accuracy: 0.5416 - loss: 1.3664 - val_accuracy: 0.3820 - val_loss: 0.9810 - learning_rate: 1.0000e-04\n",
      "Epoch 2/10\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m193s\u001b[0m 2s/step - accuracy: 0.6132 - loss: 0.8012 - val_accuracy: 0.6910 - val_loss: 0.7298 - learning_rate: 1.0000e-04\n",
      "Epoch 3/10\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m192s\u001b[0m 2s/step - accuracy: 0.6120 - loss: 0.7635 - val_accuracy: 0.5658 - val_loss: 0.9750 - learning_rate: 1.0000e-04\n",
      "Epoch 4/10\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m194s\u001b[0m 2s/step - accuracy: 0.6148 - loss: 0.7846 - val_accuracy: 0.6159 - val_loss: 0.8627 - learning_rate: 1.0000e-04\n",
      "Epoch 5/10\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m178s\u001b[0m 2s/step - accuracy: 0.6211 - loss: 0.7528 - val_accuracy: 0.3111 - val_loss: 1.8485 - learning_rate: 1.0000e-04\n",
      "Epoch 6/10\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.6660 - loss: 0.7078"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m201s\u001b[0m 3s/step - accuracy: 0.6660 - loss: 0.7076 - val_accuracy: 0.7223 - val_loss: 0.6404 - learning_rate: 3.0000e-05\n",
      "Epoch 7/10\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.6667 - loss: 0.6741"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m199s\u001b[0m 3s/step - accuracy: 0.6668 - loss: 0.6740 - val_accuracy: 0.7578 - val_loss: 0.6061 - learning_rate: 3.0000e-05\n",
      "Epoch 8/10\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m191s\u001b[0m 2s/step - accuracy: 0.6892 - loss: 0.6517 - val_accuracy: 0.5031 - val_loss: 0.8659 - learning_rate: 3.0000e-05\n",
      "Epoch 9/10\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m204s\u001b[0m 3s/step - accuracy: 0.6766 - loss: 0.6788 - val_accuracy: 0.4175 - val_loss: 1.7385 - learning_rate: 3.0000e-05\n",
      "Epoch 10/10\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m194s\u001b[0m 2s/step - accuracy: 0.7054 - loss: 0.6296 - val_accuracy: 0.4990 - val_loss: 1.1733 - learning_rate: 3.0000e-05\n",
      "✅ Saved ResNet50 model\n"
     ]
    }
   ],
   "source": [
    "# train_disease_resnet50.py\n",
    "import os, json, tensorflow as tf\n",
    "from tensorflow.keras.applications import resnet50\n",
    "from tensorflow.keras import layers, models, callbacks, optimizers\n",
    "\n",
    "DATA = r\"D:\\psm2\\MediSkinDataset\"\n",
    "OUT  = r\"D:\\psm2\\models\"; os.makedirs(OUT, exist_ok=True)\n",
    "IMG=(224,224); BATCH=32\n",
    "\n",
    "train_gen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    rescale=1./255, rotation_range=12, width_shift_range=0.05,\n",
    "    height_shift_range=0.05, zoom_range=0.15, horizontal_flip=True)\n",
    "val_gen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_ds = train_gen.flow_from_directory(f\"{DATA}\\\\train\", target_size=IMG, batch_size=BATCH, class_mode=\"categorical\")\n",
    "val_ds   = val_gen.flow_from_directory(f\"{DATA}\\\\val\",   target_size=IMG, batch_size=BATCH, class_mode=\"categorical\")\n",
    "\n",
    "with open(os.path.join(OUT,\"disease_class_indices.json\"),\"w\") as f:\n",
    "    json.dump(train_ds.class_indices, f, indent=2)\n",
    "\n",
    "base = resnet50.ResNet50(weights=\"imagenet\", include_top=False, input_shape=IMG+(3,))\n",
    "base.trainable=False\n",
    "\n",
    "model = models.Sequential([\n",
    "    base,\n",
    "    layers.GlobalAveragePooling2D(),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(256, activation=\"relu\"),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(train_ds.num_classes, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.compile(optimizer=optimizers.Adam(1e-3),\n",
    "              loss=\"categorical_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "ckp = callbacks.ModelCheckpoint(r\"D:\\psm2\\models\\disease_resnet50.h5\", save_best_only=True, monitor=\"val_accuracy\")\n",
    "es  = callbacks.EarlyStopping(patience=6, restore_best_weights=True)\n",
    "rlr = callbacks.ReduceLROnPlateau(patience=3, factor=0.3)\n",
    "\n",
    "model.fit(train_ds, epochs=25, validation_data=val_ds, callbacks=[ckp, es, rlr])\n",
    "\n",
    "# fine-tune last block\n",
    "base.trainable=True\n",
    "for layer in base.layers[:-30]:\n",
    "    layer.trainable=False\n",
    "\n",
    "model.compile(optimizer=optimizers.Adam(1e-4), loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "model.fit(train_ds, epochs=10, validation_data=val_ds, callbacks=[ckp, es, rlr])\n",
    "print(\"✅ Saved ResNet50 model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab827bcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2483 images belonging to 3 classes.\n",
      "Found 479 images belonging to 3 classes.\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "\u001b[1m87910968/87910968\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 0us/step\n",
      "Epoch 1/25\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 857ms/step - accuracy: 0.6649 - loss: 0.8359"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 1s/step - accuracy: 0.6657 - loss: 0.8336 - val_accuracy: 0.7996 - val_loss: 0.4685 - learning_rate: 0.0010\n",
      "Epoch 2/25\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 1s/step - accuracy: 0.8062 - loss: 0.4613 - val_accuracy: 0.7537 - val_loss: 0.5043 - learning_rate: 0.0010\n",
      "Epoch 3/25\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 1s/step - accuracy: 0.8500 - loss: 0.3684 - val_accuracy: 0.7871 - val_loss: 0.4977 - learning_rate: 0.0010\n",
      "Epoch 4/25\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 1s/step - accuracy: 0.8630 - loss: 0.3540 - val_accuracy: 0.7766 - val_loss: 0.4661 - learning_rate: 0.0010\n",
      "Epoch 5/25\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m96s\u001b[0m 1s/step - accuracy: 0.8727 - loss: 0.3214 - val_accuracy: 0.7557 - val_loss: 0.4794 - learning_rate: 0.0010\n",
      "Epoch 6/25\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m95s\u001b[0m 1s/step - accuracy: 0.8840 - loss: 0.2889 - val_accuracy: 0.7787 - val_loss: 0.4810 - learning_rate: 0.0010\n",
      "Epoch 7/25\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.8974 - loss: 0.2511"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m98s\u001b[0m 1s/step - accuracy: 0.8974 - loss: 0.2512 - val_accuracy: 0.8038 - val_loss: 0.4269 - learning_rate: 0.0010\n",
      "Epoch 8/25\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 1s/step - accuracy: 0.8945 - loss: 0.2566 - val_accuracy: 0.7557 - val_loss: 0.4956 - learning_rate: 0.0010\n",
      "Epoch 9/25\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 1s/step - accuracy: 0.9000 - loss: 0.2168 - val_accuracy: 0.7390 - val_loss: 0.5347 - learning_rate: 0.0010\n",
      "Epoch 10/25\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 1s/step - accuracy: 0.9187 - loss: 0.2143 - val_accuracy: 0.7704 - val_loss: 0.5442 - learning_rate: 0.0010\n",
      "Epoch 11/25\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 1s/step - accuracy: 0.9202 - loss: 0.2045 - val_accuracy: 0.7829 - val_loss: 0.4736 - learning_rate: 3.0000e-04\n",
      "Epoch 12/25\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 1s/step - accuracy: 0.9371 - loss: 0.1628 - val_accuracy: 0.7432 - val_loss: 0.5048 - learning_rate: 3.0000e-04\n",
      "Epoch 13/25\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m95s\u001b[0m 1s/step - accuracy: 0.9226 - loss: 0.1819 - val_accuracy: 0.7495 - val_loss: 0.4998 - learning_rate: 3.0000e-04\n",
      "Epoch 1/10\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 1s/step - accuracy: 0.8803 - loss: 0.3091 - val_accuracy: 0.7349 - val_loss: 0.5987 - learning_rate: 1.0000e-04\n",
      "Epoch 2/10\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 1s/step - accuracy: 0.9375 - loss: 0.1716 - val_accuracy: 0.7411 - val_loss: 0.6010 - learning_rate: 1.0000e-04\n",
      "Epoch 3/10\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 1s/step - accuracy: 0.9594 - loss: 0.1164 - val_accuracy: 0.7871 - val_loss: 0.4912 - learning_rate: 1.0000e-04\n",
      "Epoch 4/10\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m98s\u001b[0m 1s/step - accuracy: 0.9545 - loss: 0.1102 - val_accuracy: 0.7808 - val_loss: 0.5679 - learning_rate: 1.0000e-04\n",
      "Epoch 5/10\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 1s/step - accuracy: 0.9648 - loss: 0.0933 - val_accuracy: 0.8038 - val_loss: 0.5321 - learning_rate: 1.0000e-04\n",
      "Epoch 6/10\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 1s/step - accuracy: 0.9751 - loss: 0.0697 - val_accuracy: 0.7537 - val_loss: 0.5795 - learning_rate: 1.0000e-04\n",
      "Epoch 7/10\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 1s/step - accuracy: 0.9797 - loss: 0.0621 - val_accuracy: 0.7662 - val_loss: 0.5914 - learning_rate: 3.0000e-05\n",
      "Epoch 8/10\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 1s/step - accuracy: 0.9870 - loss: 0.0416 - val_accuracy: 0.7724 - val_loss: 0.6062 - learning_rate: 3.0000e-05\n",
      "Epoch 9/10\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m99s\u001b[0m 1s/step - accuracy: 0.9849 - loss: 0.0458 - val_accuracy: 0.7557 - val_loss: 0.5912 - learning_rate: 3.0000e-05\n",
      "✅ Saved InceptionV3 model\n"
     ]
    }
   ],
   "source": [
    "# train_disease_inceptionv3.py\n",
    "import os, json, tensorflow as tf\n",
    "from tensorflow.keras.applications import inception_v3\n",
    "from tensorflow.keras import layers, models, callbacks, optimizers\n",
    "\n",
    "DATA = r\"D:\\psm2\\MediSkinDataset\"\n",
    "OUT  = r\"D:\\psm2\\models\"; os.makedirs(OUT, exist_ok=True)\n",
    "IMG=(224,224); BATCH=32\n",
    "\n",
    "train_gen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    rescale=1./255, rotation_range=12, width_shift_range=0.05,\n",
    "    height_shift_range=0.05, zoom_range=0.15, horizontal_flip=True)\n",
    "val_gen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_ds = train_gen.flow_from_directory(f\"{DATA}\\\\train\", target_size=IMG, batch_size=BATCH, class_mode=\"categorical\")\n",
    "val_ds   = val_gen.flow_from_directory(f\"{DATA}\\\\val\",   target_size=IMG, batch_size=BATCH, class_mode=\"categorical\")\n",
    "\n",
    "with open(os.path.join(OUT,\"disease_class_indices.json\"),\"w\") as f:\n",
    "    json.dump(train_ds.class_indices, f, indent=2)\n",
    "\n",
    "base = inception_v3.InceptionV3(weights=\"imagenet\", include_top=False, input_shape=IMG+(3,))\n",
    "base.trainable=False\n",
    "\n",
    "model = models.Sequential([\n",
    "    base,\n",
    "    layers.GlobalAveragePooling2D(),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(256, activation=\"relu\"),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(train_ds.num_classes, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.compile(optimizer=optimizers.Adam(1e-3),\n",
    "              loss=\"categorical_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "ckp = callbacks.ModelCheckpoint(r\"D:\\psm2\\models\\disease_inceptionv3.h5\", save_best_only=True, monitor=\"val_accuracy\")\n",
    "es  = callbacks.EarlyStopping(patience=6, restore_best_weights=True)\n",
    "rlr = callbacks.ReduceLROnPlateau(patience=3, factor=0.3)\n",
    "\n",
    "model.fit(train_ds, epochs=25, validation_data=val_ds, callbacks=[ckp, es, rlr])\n",
    "\n",
    "# fine-tune tail\n",
    "base.trainable=True\n",
    "for layer in base.layers[:-30]:\n",
    "    layer.trainable=False\n",
    "\n",
    "model.compile(optimizer=optimizers.Adam(1e-4), loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "model.fit(train_ds, epochs=10, validation_data=val_ds, callbacks=[ckp, es, rlr])\n",
    "print(\"✅ Saved InceptionV3 model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bdebadf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.7.1-cp311-cp311-win_amd64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: numpy>=1.22.0 in c:\\users\\fazreen\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from scikit-learn) (2.1.3)\n",
      "Requirement already satisfied: scipy>=1.8.0 in c:\\users\\fazreen\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from scikit-learn) (1.15.3)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Downloading joblib-1.5.2-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading scikit_learn-1.7.1-cp311-cp311-win_amd64.whl (8.9 MB)\n",
      "   ---------------------------------------- 0.0/8.9 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.1/8.9 MB 3.3 MB/s eta 0:00:03\n",
      "   ---- ----------------------------------- 1.0/8.9 MB 12.2 MB/s eta 0:00:01\n",
      "   --------- ------------------------------ 2.0/8.9 MB 16.1 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 3.7/8.9 MB 21.3 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 5.3/8.9 MB 23.9 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 6.8/8.9 MB 25.6 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 8.6/8.9 MB 28.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  8.9/8.9 MB 28.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  8.9/8.9 MB 28.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 8.9/8.9 MB 21.1 MB/s eta 0:00:00\n",
      "Downloading joblib-1.5.2-py3-none-any.whl (308 kB)\n",
      "   ---------------------------------------- 0.0/308.4 kB ? eta -:--:--\n",
      "   --------------------------------------- 308.4/308.4 kB 19.9 MB/s eta 0:00:00\n",
      "Downloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, joblib, scikit-learn\n",
      "Successfully installed joblib-1.5.2 scikit-learn-1.7.1 threadpoolctl-3.6.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.2\n",
      "[notice] To update, run: C:\\Users\\Fazreen\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "292370ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 52 images belonging to 3 classes.\n",
      "\n",
      "=== MobileNetV2 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2s/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Monkeypox       0.89      0.85      0.87        20\n",
      "      Others       0.88      0.92      0.90        25\n",
      "      normal       1.00      1.00      1.00         7\n",
      "\n",
      "    accuracy                           0.90        52\n",
      "   macro avg       0.93      0.92      0.92        52\n",
      "weighted avg       0.90      0.90      0.90        52\n",
      "\n",
      "\n",
      "=== ResNet50 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3s/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Monkeypox       0.65      0.85      0.74        20\n",
      "      Others       0.83      0.76      0.79        25\n",
      "      normal       1.00      0.43      0.60         7\n",
      "\n",
      "    accuracy                           0.75        52\n",
      "   macro avg       0.83      0.68      0.71        52\n",
      "weighted avg       0.78      0.75      0.75        52\n",
      "\n",
      "\n",
      "=== InceptionV3 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000002761E455E40> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000002761E455E40> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/2\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 3s/stepWARNING:tensorflow:6 out of the last 6 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000002761E455E40> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 6 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000002761E455E40> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3s/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Monkeypox       0.83      0.95      0.88        20\n",
      "      Others       0.88      0.84      0.86        25\n",
      "      normal       1.00      0.71      0.83         7\n",
      "\n",
      "    accuracy                           0.87        52\n",
      "   macro avg       0.90      0.83      0.86        52\n",
      "weighted avg       0.87      0.87      0.86        52\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json, numpy as np, tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "DATA = r\"D:\\psm2\\MediSkinDataset\"\n",
    "IMG=(224,224)\n",
    "gen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255)\n",
    "test_ds = gen.flow_from_directory(f\"{DATA}\\\\test\", target_size=IMG, batch_size=32,\n",
    "                                  class_mode=\"categorical\", shuffle=False)\n",
    "\n",
    "models = {\n",
    "    \"MobileNetV2\": r\"D:\\psm2\\models\\disease_mnv2.h5\",\n",
    "    \"ResNet50\":    r\"D:\\psm2\\models\\disease_resnet50.h5\",\n",
    "    \"InceptionV3\": r\"D:\\psm2\\models\\disease_inceptionv3.h5\"\n",
    "}\n",
    "\n",
    "idx = json.load(open(r\"D:\\psm2\\models\\disease_class_indices.json\"))\n",
    "idx_to_cls = {v:k for k,v in idx.items()}\n",
    "\n",
    "for name, path in models.items():\n",
    "    print(f\"\\n=== {name} ===\")\n",
    "    m = load_model(path)\n",
    "    probs = m.predict(test_ds)\n",
    "    y_pred = probs.argmax(1); y_true = test_ds.classes\n",
    "    print(classification_report(y_true, y_pred,\n",
    "          target_names=[idx_to_cls[i] for i in range(len(idx_to_cls))]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ed31da8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 52 images belonging to 3 classes.\n",
      "\n",
      "=== Evaluating MobileNetV2 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "C:\\Users\\Fazreen\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Evaluating ResNet50 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Evaluating InceptionV3 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Done. Results saved to: D:\\psm2\\results\n",
      " - Confusion matrices: cm_*.png\n",
      " - Per-model reports: report_*.txt\n",
      " - CSVs: summary_overall.csv, summary_per_class.csv\n",
      " - Charts: accuracy_comparison.png, macroF1_comparison.png\n"
     ]
    }
   ],
   "source": [
    "# compare_and_plot.py\n",
    "import os, json, numpy as np, matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_recall_fscore_support\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# -------- Paths --------\n",
    "DATA_DIR = r\"D:\\psm2\\MediSkinDataset\"\n",
    "MODELS = {\n",
    "    \"MobileNetV2\":  r\"D:\\psm2\\models\\disease_mnv2.h5\",\n",
    "    \"ResNet50\":     r\"D:\\psm2\\models\\disease_resnet50.h5\",\n",
    "    \"InceptionV3\":  r\"D:\\psm2\\models\\disease_inceptionv3.h5\",\n",
    "}\n",
    "CLASS_IDX_JSON = r\"D:\\psm2\\models\\disease_class_indices.json\"\n",
    "OUT_DIR = Path(r\"D:\\psm2\\results\"); OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "IMG = (224, 224); BATCH = 32\n",
    "\n",
    "# -------- Data --------\n",
    "gen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255)\n",
    "test_ds = gen.flow_from_directory(\n",
    "    f\"{DATA_DIR}\\\\test\", target_size=IMG, batch_size=BATCH,\n",
    "    class_mode=\"categorical\", shuffle=False\n",
    ")\n",
    "\n",
    "with open(CLASS_IDX_JSON, \"r\") as f:\n",
    "    cls2idx = json.load(f)\n",
    "idx2cls = {v:k for k, v in cls2idx.items()}\n",
    "class_names = [idx2cls[i] for i in range(len(idx2cls))]\n",
    "\n",
    "# -------- Helpers --------\n",
    "def plot_confusion(cm, labels, title, save_path):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    im = ax.imshow(cm)\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(\"Predicted\")\n",
    "    ax.set_ylabel(\"True\")\n",
    "    ax.set_xticks(range(len(labels))); ax.set_xticklabels(labels, rotation=45, ha=\"right\")\n",
    "    ax.set_yticks(range(len(labels))); ax.set_yticklabels(labels)\n",
    "    for i in range(len(labels)):\n",
    "        for j in range(len(labels)):\n",
    "            ax.text(j, i, str(cm[i, j]), ha=\"center\", va=\"center\")\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(save_path, dpi=200)\n",
    "    plt.close(fig)\n",
    "\n",
    "def bar_chart(labels, values, title, ylabel, save_path):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.bar(labels, values)\n",
    "    ax.set_title(title)\n",
    "    ax.set_ylabel(ylabel)\n",
    "    ax.set_ylim(0, 1.0)\n",
    "    for i, v in enumerate(values):\n",
    "        ax.text(i, v + 0.02, f\"{v:.2f}\", ha=\"center\")\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(save_path, dpi=200)\n",
    "    plt.close(fig)\n",
    "\n",
    "# -------- Evaluate all models --------\n",
    "summary_rows = []\n",
    "per_class_rows = []\n",
    "\n",
    "for name, model_path in MODELS.items():\n",
    "    if not os.path.exists(model_path):\n",
    "        print(f\"[WARN] Missing model: {model_path} — skipping.\")\n",
    "        continue\n",
    "\n",
    "    print(f\"\\n=== Evaluating {name} ===\")\n",
    "    model = load_model(model_path)\n",
    "\n",
    "    probs = model.predict(test_ds, verbose=0)\n",
    "    y_pred = probs.argmax(axis=1)\n",
    "    y_true = test_ds.classes\n",
    "\n",
    "    # Metrics\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    precision, recall, f1, support = precision_recall_fscore_support(\n",
    "        y_true, y_pred, labels=list(range(len(class_names))), zero_division=0\n",
    "    )\n",
    "    macro_f1 = np.mean(f1)\n",
    "\n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=list(range(len(class_names))))\n",
    "\n",
    "    # Save confusion matrix image\n",
    "    cm_path = OUT_DIR / f\"cm_{name}.png\"\n",
    "    plot_confusion(cm, class_names, f\"Confusion Matrix — {name}\", cm_path)\n",
    "\n",
    "    # Save text report\n",
    "    report_dict = classification_report(\n",
    "        y_true, y_pred, target_names=class_names, zero_division=0, output_dict=True\n",
    "    )\n",
    "    report_txt = OUT_DIR / f\"report_{name}.txt\"\n",
    "    with open(report_txt, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(f\"Model: {name}\\n\\n\")\n",
    "        f.write(classification_report(y_true, y_pred, target_names=class_names, zero_division=0))\n",
    "        f.write(\"\\nConfusion Matrix:\\n\")\n",
    "        f.write(np.array2string(cm))\n",
    "\n",
    "    # Append to summary\n",
    "    summary_rows.append((name, acc, macro_f1))\n",
    "\n",
    "    # Per-class rows\n",
    "    for i, cls in enumerate(class_names):\n",
    "        per_class_rows.append((name, cls, precision[i], recall[i], f1[i], int(support[i])))\n",
    "\n",
    "# -------- Save CSVs --------\n",
    "import csv\n",
    "summary_csv = OUT_DIR / \"summary_overall.csv\"\n",
    "with open(summary_csv, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    w = csv.writer(f); w.writerow([\"Model\", \"Accuracy\", \"Macro_F1\"])\n",
    "    for row in summary_rows:\n",
    "        w.writerow(row)\n",
    "\n",
    "perclass_csv = OUT_DIR / \"summary_per_class.csv\"\n",
    "with open(perclass_csv, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    w = csv.writer(f); w.writerow([\"Model\", \"Class\", \"Precision\", \"Recall\", \"F1\", \"Support\"])\n",
    "    for row in per_class_rows:\n",
    "        w.writerow(row)\n",
    "\n",
    "# -------- Comparison plots --------\n",
    "# Overall Accuracy\n",
    "labels = [r[0] for r in summary_rows]\n",
    "accs   = [r[1] for r in summary_rows]\n",
    "bar_chart(labels, accs, \"Overall Accuracy Comparison\", \"Accuracy\", OUT_DIR / \"accuracy_comparison.png\")\n",
    "\n",
    "# Overall Macro-F1\n",
    "mfs = [r[2] for r in summary_rows]\n",
    "bar_chart(labels, mfs, \"Macro-F1 Comparison\", \"Macro-F1\", OUT_DIR / \"macroF1_comparison.png\")\n",
    "\n",
    "print(\"\\n✅ Done. Results saved to:\", OUT_DIR)\n",
    "print(\" - Confusion matrices: cm_*.png\")\n",
    "print(\" - Per-model reports: report_*.txt\")\n",
    "print(\" - CSVs: summary_overall.csv, summary_per_class.csv\")\n",
    "print(\" - Charts: accuracy_comparison.png, macroF1_comparison.png\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
